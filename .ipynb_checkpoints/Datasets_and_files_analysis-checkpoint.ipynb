{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import hvplot.pandas\n",
    "import holoviews as hv\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import holoviews.plotting.mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from holoviews import opts\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import igraph as ig\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_numeric(df):\n",
    "    df = df[df.amount.apply(lambda x: str(x).strip().isnumeric())]\n",
    "    df.loc[:, 'amount'] = df.amount.astype(int)\n",
    "    return df\n",
    "\n",
    "def normal_open(text):\n",
    "    lst = open(text, 'r', encoding='utf-8').read().split('\\n')\n",
    "    df = pd.DataFrame([p.strip().split(' ', 1) for p in lst], columns=['amount', 'text'])\n",
    "    return only_numeric(df)\n",
    "\n",
    "def txt2df(text):\n",
    "    try:\n",
    "        tbl = pd.read_csv(text, sep='\\t')\n",
    "        if tbl.columns.__len__() < 2:\n",
    "            try:\n",
    "                return normal_open(text)\n",
    "            except:\n",
    "                if text == 'D:\\\\graphs\\\\project\\\\disasters_vs_politics\\\\disasters\\\\dorian\\\\dorian-media.txt':\n",
    "                    tbl = pd.read_csv(text, header=None, sep=' ', names=['amount', 'text'])\n",
    "                    return pd.DataFrame(tbl.amount.value_counts().reset_index(), columns=['index', 'amount']).rename({'index':'text'})\n",
    "                else:\n",
    "                    tbl = pd.read_csv(text, sep=' ', header=None, names=['id', 'text']).id.value_counts().reset_index()\n",
    "                    tbl.columns = ['text', 'amount']\n",
    "                    return tbl\n",
    "        else:\n",
    "            if text == 'D:\\\\graphs\\\\project\\\\disasters_vs_politics\\\\disasters\\\\florence\\\\HurricaneFlorence-media.txt':\n",
    "                tbl = pd.read_csv(text, sep='\\t', names=['id', 'text']).id.value_counts().reset_index()\n",
    "                tbl.columns = ['text', 'amount']\n",
    "                return tbl\n",
    "            else:\n",
    "                return pd.read_csv(text, sep='\\t', names=['amount', 'text'])\n",
    "    except:\n",
    "        return normal_open(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Users Distributions Disasters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_disasters_users = {\n",
    "    'Dallas Police Shooting Disaster':\n",
    "        'D:\\\\graphs\\\\project\\\\disasters_vs_politics\\\\disasters\\\\dallas_police_shooting\\\\dallas_police_shooting_users.txt'\n",
    "    ,\n",
    "    'Dorian Disaster':\n",
    "        'D:\\\\graphs\\\\project\\\\disasters_vs_politics\\\\disasters\\\\dorian\\\\dorian-users.txt'\n",
    "    ,\n",
    "    'Florence Disaster':\n",
    "        'D:\\\\graphs\\\\project\\\\disasters_vs_politics\\\\disasters\\\\florence\\\\HurricaneFlorence-users.txt'\n",
    "    ,\n",
    "    'Harvey Disaster':\n",
    "        'D:\\\\graphs\\\\project\\\\disasters_vs_politics\\\\disasters\\\\harvey\\\\HurricaneHarvey_users.txt'\n",
    "    ,\n",
    "    'Imelda Disaster':\n",
    "        'D:\\\\graphs\\\\project\\\\disasters_vs_politics\\\\disasters\\\\imelda\\\\imelda-users.txt'\n",
    "    ,\n",
    "    'Notre Dam Fire Disaster':\n",
    "        'D:\\\\graphs\\\\project\\\\disasters_vs_politics\\\\disasters\\\\notre_dam_fire\\\\notre-dame-fire-users.txt'\n",
    "    }\n",
    "\n",
    "paths_disasters_users_csvs = {\n",
    "    'Dallas Police Shooting Disaster':\n",
    "        'D:\\\\graphs\\\\project\\\\disasters_vs_politics\\\\disasters\\\\dallas_police_shooting\\\\dallas_police_shooting_users.csv'\n",
    "    ,\n",
    "    'Dorian Disaster':\n",
    "        'D:\\\\graphs\\\\project\\\\disasters_vs_politics\\\\disasters\\\\dorian\\\\dorian-users.csv'\n",
    "    ,\n",
    "    'Florence Disaster':\n",
    "        'D:\\\\graphs\\\\project\\\\disasters_vs_politics\\\\disasters\\\\florence\\\\HurricaneFlorence-users.csv'\n",
    "    ,\n",
    "    'Harvey Disaster':\n",
    "        'D:\\\\graphs\\\\project\\\\disasters_vs_politics\\\\disasters\\\\harvey\\\\HurricaneHarvey_users.csv'\n",
    "    ,\n",
    "    'Imelda Disaster':\n",
    "        'D:\\\\graphs\\\\project\\\\disasters_vs_politics\\\\disasters\\\\imelda\\\\imelda-users.csv'\n",
    "    ,\n",
    "    'Notre Dam Fire Disaster':\n",
    "        'D:\\\\graphs\\\\project\\\\disasters_vs_politics\\\\disasters\\\\notre_dam_fire\\\\notre-dame-fire-users.csv'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meta_data(name):\n",
    "    fileharvey = paths_disasters_users_csvs.get(name)\n",
    "    df = pd.read_csv(fileharvey)\n",
    "    meta_data = ('disaster, ',\n",
    "              'aggregative, ',\n",
    "              name+', ',\n",
    "              'digital libary, ',\n",
    "              str(df.shape[0])+', ',\n",
    "              '0, ',\n",
    "             str(round(os.path.getsize(fileharvey)/1000))+'KB',\n",
    "             '0')\n",
    "    return meta_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('disaster, ', 'aggregative, ', 'Dallas Police Shooting Disaster, ', 'digital libary, ', '2287421, ', '0, ', '24170KB', '0')\n",
      "('disaster, ', 'aggregative, ', 'Dorian Disaster, ', 'digital libary, ', '1079699, ', '0, ', '10816KB', '0')\n",
      "('disaster, ', 'aggregative, ', 'Florence Disaster, ', 'digital libary, ', '1709988, ', '0, ', '17777KB', '0')\n",
      "('disaster, ', 'aggregative, ', 'Harvey Disaster, ', 'digital libary, ', '2137670, ', '0, ', '22526KB', '0')\n",
      "('disaster, ', 'aggregative, ', 'Imelda Disaster, ', 'digital libary, ', '40502, ', '0, ', '354KB', '0')\n",
      "('disaster, ', 'aggregative, ', 'Notre Dam Fire Disaster, ', 'digital libary, ', '3745399, ', '0, ', '40179KB', '0')\n"
     ]
    }
   ],
   "source": [
    "for name in list(paths_disasters_users.keys()):\n",
    "    print(meta_data(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### politics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_politics = {\n",
    "    'Bangladesh Politics':\n",
    "        'D:\\\\graphs\\\\project\\\\disasters_vs_politics\\\\politics_users\\\\bangladesh_linked_tweets_csv_hashed.csv'\n",
    ",\n",
    "    'China Politics 1':\n",
    "        'D:\\\\graphs\\\\project\\\\disasters_vs_politics\\\\politics_users\\\\china_082019_1_tweets_csv_hashed.csv'\n",
    ",\n",
    "    'China Politics 2':\n",
    "        'D:\\\\graphs\\\\project\\\\disasters_vs_politics\\\\politics_users\\\\china_082019_2_tweets_csv_hashed.csv'\n",
    ",\n",
    "    'Ecuador Politics':\n",
    "        'D:\\\\graphs\\\\project\\\\disasters_vs_politics\\\\politics_users\\\\ecuador_082019_tweets_csv_hashed.csv'\n",
    ",\n",
    "    'Egypt Politics':\n",
    "        'D:\\\\graphs\\\\project\\\\disasters_vs_politics\\\\politics_users\\\\egypt_uae_082019_tweets_csv_hashed.csv'\n",
    ",\n",
    "    'Iran Politics':\n",
    "        'D:\\\\graphs\\\\project\\\\disasters_vs_politics\\\\politics_users\\\\iranian_tweets_csv_hashed.csv'\n",
    ",\n",
    "    'Russia Politics':\n",
    "        'D:\\\\graphs\\\\project\\\\disasters_vs_politics\\\\politics_users\\\\russian_linked_tweets_csv_hashed.csv'\n",
    ",\n",
    "    'Spain Politics':\n",
    "        'D:\\\\graphs\\\\project\\\\disasters_vs_politics\\\\politics_users\\\\spain_082019_tweets_csv_hashed.csv'\n",
    ",\n",
    "    'Venezuela Politics':\n",
    "        'D:\\\\graphs\\\\project\\\\disasters_vs_politics\\\\politics_users\\\\venezuela_linked_tweets_csv_hashed.csv'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### political twitter data from\n",
    "### https://www.kaggle.com/paultimothymooney/twitter-election-data-archives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_politic_df(file):\n",
    "    df = pd.read_csv(file, usecols=['userid', 'tweetid'], dtype=str)\n",
    "    return df\n",
    "\n",
    "def meta_politics(name):   \n",
    "    fileharvey = paths_politics.get(name)\n",
    "    df = get_politic_df(fileharvey)\n",
    "    meta_data = ('politics',\n",
    "              'original',\n",
    "              name,\n",
    "              'Kaggle',\n",
    "              str(df.shape[0]),\n",
    "              str(df.tweetid.drop_duplicates().count()),\n",
    "             str(round(os.path.getsize(fileharvey)/1000))+'KB')\n",
    "    return meta_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('politics', 'original', 'Bangladesh Politics', 'Kaggle', '26212', '26212', '18761KB')\n",
      "('politics', 'original', 'China Politics 1', 'Kaggle', '1898108', '1898108', '759418KB')\n",
      "('politics', 'original', 'China Politics 2', 'Kaggle', '1708078', '1708075', '807932KB')\n",
      "('politics', 'original', 'Ecuador Politics', 'Kaggle', '700240', '700240', '402335KB')\n",
      "('politics', 'original', 'Egypt Politics', 'Kaggle', '214898', '214898', '169600KB')\n",
      "('politics', 'original', 'Iran Politics', 'Kaggle', '1122936', '1122936', '699663KB')\n",
      "('politics', 'original', 'Russia Politics', 'Kaggle', '920761', '920761', '601661KB')\n",
      "('politics', 'original', 'Spain Politics', 'Kaggle', '56712', '56712', '34156KB')\n",
      "('politics', 'original', 'Venezuela Politics', 'Kaggle', '984980', '984980', '604522KB')\n"
     ]
    }
   ],
   "source": [
    "for name in list(paths_politics.keys()):\n",
    "    print(meta_politics(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_numeric(df):\n",
    "    df = df[df.amount.apply(lambda x: str(x).strip().isnumeric())]\n",
    "    df.loc[:, 'amount'] = df.amount.astype(int)\n",
    "    return df\n",
    "\n",
    "\n",
    "def normal_open(text):\n",
    "    lst = open(text, 'r', encoding='utf-8').read().split('\\n')\n",
    "    df = pd.DataFrame([p.strip().split(' ', 1) for p in lst], columns=['amount', 'text'])\n",
    "    return only_numeric(df)\n",
    "\n",
    "def txt2df(text):\n",
    "    try:\n",
    "        tbl = pd.read_csv(text, sep='\\t')\n",
    "        if tbl.columns.__len__() < 2:\n",
    "            try:\n",
    "                return normal_open(text)\n",
    "            except:\n",
    "                if text == 'D:\\\\graphs\\\\project\\\\disasters_vs_politics\\\\disasters\\\\dorian\\\\dorian-media.txt':\n",
    "                    tbl = pd.read_csv(text, header=None, sep=' ', names=['amount', 'text'])\n",
    "                    return pd.DataFrame(tbl.amount.value_counts().reset_index(), columns=['index', 'amount']).rename({'index':'text'})\n",
    "                else:\n",
    "                    tbl = pd.read_csv(text, sep=' ', header=None, names=['id', 'text']).id.value_counts().reset_index()\n",
    "                    tbl.columns = ['text', 'amount']\n",
    "                    return tbl\n",
    "        else:\n",
    "            if text == 'D:\\\\graphs\\\\project\\\\disasters_vs_politics\\\\disasters\\\\florence\\\\HurricaneFlorence-media.txt':\n",
    "                tbl = pd.read_csv(text, sep='\\t', names=['id', 'text']).id.value_counts().reset_index()\n",
    "                tbl.columns = ['text', 'amount']\n",
    "                return tbl\n",
    "            else:\n",
    "                return pd.read_csv(text, sep='\\t', names=['amount', 'text'])\n",
    "    except:\n",
    "        return normal_open(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hashtag Distributions Disasters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# disasters twitter data from \n",
    "https://digital.library.unt.edu/ark:/67531/metadc1706014/m1/ - dorian\n",
    "\n",
    "https://digital.library.unt.edu/ark:/67531/metadc1259406/m1/?q=twitter - HurricaneFlorence\n",
    "\n",
    "https://digital.library.unt.edu/ark:/67531/metadc993940/m1/?q=twitter - HurricaneHarvey\n",
    "\n",
    "https://digital.library.unt.edu/ark:/67531/metadc1706012/m1/?q=twitter - imelda\n",
    "\n",
    "https://digital.library.unt.edu/ark:/67531/metadc1477117/m1/?q=tweets - notre-dame-fire\n",
    "\n",
    "https://digital.library.unt.edu/ark:/67531/metadc991469/m1/?q=tweets - dallas_police_shooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_disasters_hashtags = {\n",
    "        'Dallas Police Shooting Disaster':\n",
    "            'D:\\\\graphs\\\\project\\\\disasters_vs_politics\\\\disasters\\\\dallas_police_shooting\\\\dallas_police_shooting_hashtags.txt'\n",
    "    ,\n",
    "        'Dorian Disaster':\n",
    "            'D:\\\\graphs\\\\project\\\\disasters_vs_politics\\\\disasters\\\\dorian\\\\dorian-hashtags.txt'\n",
    "    ,\n",
    "        'Florence Disaster':\n",
    "            'D:\\\\graphs\\\\project\\\\disasters_vs_politics\\\\disasters\\\\florence\\\\HurricaneFlorence-hashtags.txt'\n",
    "    ,\n",
    "        'Harvey Disaster':\n",
    "            'D:\\\\graphs\\\\project\\\\disasters_vs_politics\\\\disasters\\\\harvey\\\\HurricaneHarvey_hashtags.txt'\n",
    "    ,\n",
    "        'Imelda Disaster':\n",
    "            'D:\\\\graphs\\\\project\\\\disasters_vs_politics\\\\disasters\\\\imelda\\\\imelda-hashtags.txt'\n",
    "    ,\n",
    "        'Notre Dam Fire Disaster':\n",
    "            'D:\\\\graphs\\\\project\\\\disasters_vs_politics\\\\disasters\\\\notre_dam_fire\\\\notre-dame-fire-hashtags.txt'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing_simple(filename):\n",
    "    df = txt2df(filename)\n",
    "    df = df.reset_index()\n",
    "    df =df.rename(columns={'index':'hashtag_index'})\n",
    "    df.hashtag_index = df.hashtag_index+1\n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocessing_complex(filename):\n",
    "    df = txt2df(filename)\n",
    "    df = df.loc[1:]\n",
    "    df.amount = df.amount.astype(int)\n",
    "    df = df.sort_values(ascending=False, by=['amount'])\n",
    "    df['text'] = list(range(1,df.shape[0]+1))\n",
    "    df =df.rename(columns={'text':'hashtag_index'})\n",
    "    return df\n",
    "\n",
    "\n",
    "def plot_regular(name):\n",
    "    fileharvey = paths_disasters_hashtags.get(name)\n",
    "    df = pre_processing_simple(fileharvey)\n",
    "    meta_data = ('disaster',\n",
    "                'aggregative',\n",
    "                  name,\n",
    "                  'digital libary',\n",
    "              str(df.shape[0]),\n",
    "              str(df.text.drop_duplicates().count()),\n",
    "             str(round(os.path.getsize(fileharvey)/1000))+'KB')\n",
    "    return meta_data\n",
    "\n",
    "\n",
    "\n",
    "def plot_complex(name):\n",
    "    fileharvey = paths_disasters_hashtags.get(name)\n",
    "    df = preprocessing_complex(fileharvey)\n",
    "    meta_data = ('disaster',\n",
    "                'aggregative',\n",
    "                  name,\n",
    "              'digital libary',\n",
    "          str(df.shape[0]),\n",
    "          str(df.text.drop_duplicates().count()),\n",
    "         str(round(os.path.getsize(fileharvey)/1000))+'KB')\n",
    "    return meta_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('disaster', 'aggregative', 'Dallas Police Shooting Disaster', 'digital libary', '89922', '89907', '1371KB')\n",
      "('disaster', 'aggregative', 'Dorian Disaster', 'digital libary', '35044', '35044', '490KB')\n",
      "('disaster', 'aggregative', 'Florence Disaster', 'digital libary', '101083', '101082', '1485KB')\n",
      "('disaster', 'aggregative', 'Harvey Disaster', 'digital libary', '133106', '133106', '2649KB')\n",
      "('disaster', 'aggregative', 'Imelda Disaster', 'digital libary', '2339', '2339', '31KB')\n",
      "('disaster', 'aggregative', 'Notre Dam Fire Disaster', 'digital libary', '40578', '40577', '570KB')\n"
     ]
    }
   ],
   "source": [
    "for name in list(paths_disasters_hashtags.keys()):\n",
    "    try:\n",
    "        print(plot_regular(name))\n",
    "    except:\n",
    "        print(plot_complex(name))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Politics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_politics = {\n",
    "        'Usa 2016 Politics':\n",
    "            'D:\\\\graphs\\\\project\\\\disasters_vs_politics\\\\politics\\\\2016_US_election_tweets_100k.csv'\n",
    "    ,\n",
    "        'Bangladesh 2019 Politics':\n",
    "            'D:\\\\graphs\\\\project\\\\disasters_vs_politics\\\\politics\\\\bangladesh_linked_tweets_csv_2019.csv'\n",
    "    ,\n",
    "        'Catalonia 2006 Politics':\n",
    "            'D:\\\\graphs\\\\project\\\\disasters_vs_politics\\\\politics\\\\catalonia_201906_1_tweets_csv_hashed.csv'\n",
    "    ,\n",
    "        'China 2019 Politics 1':\n",
    "            'D:\\\\graphs\\\\project\\\\disasters_vs_politics\\\\politics\\\\china_082019_1_tweets_csv_election.csv'\n",
    "    ,\n",
    "        'China 2019 Politics 2':\n",
    "            'D:\\\\graphs\\\\project\\\\disasters_vs_politics\\\\politics\\\\china_082019_2_tweets_csv_election.csv'\n",
    "    ,\n",
    "        'Ecuador 2019 Politics':\n",
    "            'D:\\\\graphs\\\\project\\\\disasters_vs_politics\\\\politics\\\\ecuador_082019_tweets_csv_hashed.csv'\n",
    "    ,\n",
    "        'Egypt 2019 Politics':\n",
    "            'D:\\\\graphs\\\\project\\\\disasters_vs_politics\\\\politics\\\\egypt_uae_082019_tweets_csv_hashed.csv'\n",
    "    ,\n",
    "        'Usa 2011 Politics 1':\n",
    "            'D:\\\\graphs\\\\project\\\\disasters_vs_politics\\\\politics\\\\elections_202011_p01.csv'\n",
    "    ,\n",
    "        'Usa 2011 Politics 2':\n",
    "            'D:\\\\graphs\\\\project\\\\disasters_vs_politics\\\\politics\\\\elections_202011_p02.csv'\n",
    "    ,\n",
    "        'Iran 2018 Politics':\n",
    "            'D:\\\\graphs\\\\project\\\\disasters_vs_politics\\\\politics\\\\iranian_tweets_csv_election2018.csv'\n",
    "    ,\n",
    "        'Russia 2019 Politics':\n",
    "            'D:\\\\graphs\\\\project\\\\disasters_vs_politics\\\\politics\\\\russian_linked_tweets_csv_2019.csv'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# political twitter data from \n",
    "https://www.kaggle.com/paultimothymooney/twitter-election-data-archives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hashtag distributions politics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hashtags_count(df, col):\n",
    "    df.loc[:, 'Hashtags'] = df.loc[:, df.columns[0]].apply(lambda x: list(re.findall(r\"#(\\w+)\", str(x))) if len(re.findall(r\"#(\\w+)\", str(x)))>0 else None)\n",
    "    df = df.dropna(subset=['Hashtags'], inplace=False)    \n",
    "    dd = pd.concat([pd.Series(row[df.columns[0]], row['Hashtags']) for _, row in df.iterrows()]).reset_index().drop_duplicates().rename(columns={0:'text', 'index':'hashtag'}) \n",
    "    result = dd.hashtag.value_counts().reset_index().rename(columns={'hashtag':'amount', 'index':'hashtag_index'})\n",
    "    return result, df[col].drop_duplicates().count()\n",
    "\n",
    "def get_politic_df(file):\n",
    "    col_names_lst = ['Text', 'text', 'tweet', 'full_text', 'tweet_text', 'Tweet Text', 'Tweet_Text', 'Orig_Tweet', 'Tweet']\n",
    "    for column in col_names_lst:\n",
    "        try:\n",
    "            df = pd.read_csv(file, usecols=[column])\n",
    "            return get_hashtags_count(df, column)\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "def plot_politics(name):\n",
    "    fileharvey = paths_politics.get(name)\n",
    "    df, text_count = get_politic_df(fileharvey)\n",
    "    meta_data = ('politics',\n",
    "            'original',\n",
    "              name,\n",
    "          'Kaggle',\n",
    "          str(df.shape[0]),\n",
    "          str(text_count),\n",
    "         str(round(os.path.getsize(fileharvey)/1000))+'KB')\n",
    "    return meta_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('politics', 'original', 'Usa 2016 Politics', 'Kaggle', '7151', '11463', '16068KB')\n",
      "('politics', 'original', 'Bangladesh 2019 Politics', 'Kaggle', '983', '1724', '18761KB')\n",
      "('politics', 'original', 'Catalonia 2006 Politics', 'Kaggle', '875', '1478', '6210KB')\n",
      "('politics', 'original', 'China 2019 Politics 1', 'Kaggle', '26487', '350472', '759418KB')\n",
      "('politics', 'original', 'China 2019 Politics 2', 'Kaggle', '78028', '293545', '807932KB')\n",
      "('politics', 'original', 'Ecuador 2019 Politics', 'Kaggle', '36280', '116707', '402335KB')\n",
      "('politics', 'original', 'Egypt 2019 Politics', 'Kaggle', '35486', '157474', '169600KB')\n",
      "('politics', 'original', 'Usa 2011 Politics 1', 'Kaggle', '22241', '133823', '218818KB')\n",
      "('politics', 'original', 'Usa 2011 Politics 2', 'Kaggle', '53757', '454174', '907758KB')\n",
      "('politics', 'original', 'Iran 2018 Politics', 'Kaggle', '111799', '276560', '699663KB')\n",
      "('politics', 'original', 'Russia 2019 Politics', 'Kaggle', '92814', '339224', '601661KB')\n"
     ]
    }
   ],
   "source": [
    "for name in list(paths_politics.keys()):\n",
    "    print(plot_politics(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bipartite Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disaster_path = 'D:\\\\datasets\\\\5100460\\\\twitter-events-2012-2016'\n",
    "\n",
    "disaster_files = [('2012-hurricane-sandy.jsonl', 'hurricane-sandy'),\n",
    "         ('2013-boston-marathon-bombing.jsonl', 'boston-marathon-bombing'),\n",
    "         ('2014-ottawashooting.jsonl', 'ottawashooting'),\n",
    "         ('2014-typhoon-hagupit.jsonl', 'typhoon-hagupit'),\n",
    "         ('2015-hurricanepatricia.jsonl', 'hurricane-patricia'),\n",
    "         ('2015-nepalearthquake.jsonl', 'nepalearthquake')]\n",
    "\n",
    "politics_path = \"D:\\\\datasets\\\\5100460\\\\twitter-events-2012-2016\\\\politics\"\n",
    "\n",
    "politics_files = os.listdir(politics_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_political_content(csv_path, file):\n",
    "    path = csv_path + \"\\\\\" + file\n",
    "    col_names = ['Text', 'text', 'tweet_text', 'tweet_text', 'Tweet Text', 'Tweet_Text', 'Orig_Tweet', 'tweet', 'Tweet']\n",
    "    k = 200000\n",
    "    for name in col_names:\n",
    "        try:\n",
    "            df_of_tweets_text = pd.read_csv(path, usecols=[name]).sample(n=k, replace=True)\n",
    "            break\n",
    "        except:\n",
    "            continue\n",
    "    list_text = df_of_tweets_text.drop_duplicates()[name].to_list()\n",
    "    list_text = [str(x) for x in list_text]\n",
    "    print('read ', len(list_text), file)\n",
    "    return list_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(input_path) -> list:\n",
    "    \"\"\"\n",
    "    Read list of objects from a JSON lines file.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        while i < 200:\n",
    "            i=0\n",
    "            for line in f:\n",
    "                try:\n",
    "                    data.append(json.loads(line.rstrip('\\n|\\r')))\n",
    "                except:\n",
    "                    i+=1\n",
    "                    continue\n",
    "        print('Loaded {} records from {}'.format(len(data), input_path))\n",
    "        print(i)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = disaster_files[0][0]\n",
    "name = disaster_files[0][1]\n",
    "document = load_jsonl(disaster_path+'\\\\'+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
